<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
<h2>Neural Network</h2>
<p>One of the major weaknesses of perceptron is that they are linear! Namely, they are unable to learn arbitrary decision boundaries. One approach of solving this is to chain together a collection of perceptrons to build a <i>Neural Network</i>. An example of a neural network is shown in the figure below. Here, you can see 2 inputs (features) that are fed into 4 hidden units. These hidden units are then fed in to 2 output units. Each edge in this figure corresponds to a different weight. Please note that each unit in hidden layer and output layer also gets a bias term which I have omitted in the figure to make it look clean.</p>
<img style="max-width: 100%;" src="/images/neural_network.png"/>
<p>So what does the neuron exactly do? It calculates a “weighted sum” of its input, adds a bias and then the activation function decides whether it should be “fired” or not. There are various types of activation functions, you can find a detailed list <a href="https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions">here</a>. The value of output Y from the neuron can be anything ranging from -inf to +inf depending on the activation function. Consider \(f(x)\) is the activation function then the output from the neuron would be - </p>
$$Y = f(\sum wx + b )$$
<p>Activation functions are the sole reason why our Neural Network can fit non-linear data. Consider the network without activation functions, the final output would be a bunch of matrix multiplications \(Y = W3 (W2 * (W1 * x)) = W * x\) which is linear.</p>
<p>Once we calculate our final output \(\hat{Y}\) at the output layer, we use loss function to find how good/bad this output is with respect to our true output Y. It basically defines how much to penalize the model for wrong predictions. There are various loss functions, some of which are listed <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">here</a>. If \(l(x)\) is some loss function the the loss L is defined as - </p>
$$L = l(\hat{Y} , Y)$$
<p>I hope now you have an brief overview of what the Neural network is doing. Let's look into the detail the code and the math behind training of the model. </p>
<h3>Training Neural Network</h3>
<p>Training a Neural Network means to find the set of weights W and biases b such that the model has the minimum loss from our loss function. Lets train our model on the following non-linearly distributed data points - </p>
<pre style="overflow:scroll;" class="prettyprint">
np.random.seed(0)
X, y = sklearn.datasets.make_moons(200, noise=0.20)
plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)
</pre>
