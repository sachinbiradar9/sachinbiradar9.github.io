<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<h2>Neural Network</h2>
<p>One of the major weaknesses of perceptron is that they are linear! Namely, they are unable to learn arbitrary decision boundaries. One approach of solving this is to chain together a collection of perceptrons to build a <i>Neural Network</i>. An example of a neural network is shown in the figure below. Here, you can see 2 inputs (features) that are fed into 4 hidden units. These hidden units are then fed in to 2 output units. Each edge in this figure corresponds to a different weight. Please note that each unit in hidden layer and output layer also gets a bias term which I have omitted in the figure to make it look clean.</p>
<img max-width="100%" src="/images/neural_network.png"/>
<p>So what does the neuron exactly do? It calculates a “weighted sum” of its input, adds a bias and then the activation function decides whether it should be “fired” or not. There are various types of activation functions, you can find a detailed list <a href="https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions">here</a>. Consider \(f(x)\) is the activation function then the output from the neuron would be -  
$$Y = f(\sum wx + b )$$
  
Now, the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know the bounds of the value. So how do we decide whether the neuron should fire or not ( why this firing pattern? Because we learnt it from biology that’s the way brain works and brain is a working testimony of an awesome and intelligent system ).
We decided to add “activation functions” for this purpose. To check the Y value produced by a neuron and decide whether outside connections should consider this neuron as “fired” or not. Or rather let’s say — “activated” or not. </p>


